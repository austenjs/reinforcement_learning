{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\users\\auste\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\auste\\anaconda3\\lib\\site-packages (from gym[atari]) (1.18.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\auste\\anaconda3\\lib\\site-packages (from gym[atari]) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\auste\\anaconda3\\lib\\site-packages (from gym[atari]) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\auste\\anaconda3\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\auste\\anaconda3\\lib\\site-packages (from gym[atari]) (7.0.0)\n",
      "Collecting atari-py~=0.2.0; extra == \"atari\"\n",
      "  Downloading atari_py-0.2.6-cp37-cp37m-win_amd64.whl (1.8 MB)\n",
      "Requirement already satisfied: opencv-python>=3.; extra == \"atari\" in c:\\users\\auste\\anaconda3\\lib\\site-packages (from gym[atari]) (4.5.1.48)\n",
      "Requirement already satisfied: future in c:\\users\\auste\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\users\\auste\\anaconda3\\lib\\site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.14.0)\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-0.2.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym\n",
    "#!pip install gym[atari]\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\") #environment info\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(22):\n",
    "    #The ball is released after 20 frames\n",
    "    if i > 20:\n",
    "        plt.imshow(observation)\n",
    "        plt.show()\n",
    "    #Get the next observation\n",
    "    observation, _, _, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    #remove top part of frame and some background\n",
    "    frame = frame[35:195, 10:150]\n",
    "    #grayscale frame and downsize by factor 2\n",
    "    frame = frame[::2,::2,0]\n",
    "    #set background to 0\n",
    "    frame[frame == 144] = 0\n",
    "    frame[frame == 109] = 0\n",
    "    #set ball and paddles to 1\n",
    "    frame[frame != 0] = 1\n",
    "    return frame.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_preprocessed = preprocess_frame(observation).reshape(80,70)\n",
    "plt.imshow(obs_preprocessed, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAAD7CAYAAABzPJi5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAALqUlEQVR4nO3dYYhl5X3H8e+vu0q6SXXURllcWxXE6BvXdEkVS0m1FpuK5kUsSlokCPoiLQopieaNCi2YN4l5UYJBTfeFjVoTiUgwkY2hLYSNmrVNdLUaa3VY49rExTQDKZv8++Ie62Bnumfmzsw/M/P9wOXe5zn3cp4zZ397zrlz5v+kqpDU49e6ByBtZgZQamQApUYGUGpkAKVGBlBqNFUAk1yS5LkkLyS5caUGJW0WWe7vAZNsAf4NuBiYBR4HrqqqZ1ZueNLGtnWKz34AeKGqXgRIci9wObBoALdt21YzMzNTrFJafw4dOsTc3FwWWjZNAE8GXpnXngV+9//7wMzMDNddd90Uq5TWnzvuuGPRZdNcAy6U6P9zPpvk2iRPJHlibm5uitVJG880AZwFTpnX3gEceOebquqLVbWrqnZt27ZtitVJG880AXwcOCPJaUmOBq4EHlqZYUmbw7KvAavqcJK/AL4BbAHurqqnV2xk0iYwzZcwVNXXga+v0FikTcc7YaRGBlBqZAClRgZQamQApUYGUGpkAKVGBlBqZAClRgZQamQApUYGUGpkAKVGBlBqZAClRgZQamQApUYGUGp0xAAmuTvJwSQ/mNd3fJJHkzw/PB+3usOUNqYxR8C/Ay55R9+NwJ6qOgPYM7QlLdERA1hV/wj85B3dlwO7h9e7gQ+v8LikTWG514AnVdWrAMPziSs3JGnzWPUvYSxNLy1uuQF8Lcl2gOH54GJvtDS9tLjlBvAh4Orh9dXA11ZmONLmMubXEF8GvgOcmWQ2yTXAbcDFSZ5nMkHnbas7TGljOmJp+qq6apFFF63wWKRNxzthpEYGUGpkAKVGBlBqZAClRgZQamQApUYGUGpkAKVGBlBqZAClRgZQamQApUYGUGpkAKVGBlBqZAClRgZQamQApUZjijKdkuSxJPuTPJ3k+qHf+SGkKY05Ah4GPlFVZwHnAR9PcjbODyFNbczcEK9W1feG1z8F9gMn4/wQ0tSWdA2Y5FTgXGAvI+eHsDS9tLjRAUzyHuArwA1V9ebYz1maXlrcqAAmOYpJ+O6pqq8O3aPnh5C0sDHfgga4C9hfVZ+dt8j5IaQpHbE0PXAB8OfA95M8NfR9msl8EPcPc0W8DFyxOkOUNq4xc0P8M5BFFjs/hDQF74SRGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqZEBlBoZQKmRAZQaGUCp0ZiiTO9K8t0k/zKUpr916D8tyd6hNP19SY5e/eFKG8uYI+DPgQur6hxgJ3BJkvOAzwCfG0rTvwFcs3rDlDamMaXpq6r+a2geNTwKuBB4YOi3NL20DGML824ZShIeBB4FfggcqqrDw1tmmcwXsdBnLU0vLWJUAKvqF1W1E9gBfAA4a6G3LfJZS9NLi1jSt6BVdQj4NpNpymaSvFVXdAdwYGWHJm18Y74FfW+SmeH1rwN/yGSKsseAjwxvszS9tAxjStNvB3Yn2cIksPdX1cNJngHuTfLXwD4m80dIWoIxpen/lcmcgO/sf5HJ9aCkZfJOGKmRAZQaGUCpkQGUGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqZEBlBqNDuBQG3RfkoeHtqXppSkt5Qh4PZNqaG+xNL00pbGVsXcAfwLcObSDpemlqY09At4OfBL45dA+AUvTS1MbU5j3UuBgVT05v3uBt1qaXlqiMYV5LwAuS/Ih4F3AMUyOiDNJtg5HwRUpTX/LLbcsqV8Lm//z8mf3q23M9GQ3VdWOqjoVuBL4VlV9FEvTS1MbcwRczKdY4dL0/m+9fDfffPP/vvbnuH4sKYBV9W0msyNZml5aAd4JIzWa5hRUv6I8BV0/PAJKjQyg1MhT0A3i1ltv7R6ClsEjoNTIAEqNDKDUyABKjQyg1MgASo0MoNTIAEqNDKDUyABKjQyg1MgASo0MoNRo1F9DJHkJ+CnwC+BwVe1KcjxwH3Aq8BLwp1X1xjSDmV/XZD7v9NdGtZQj4B9U1c6q2jW0bwT2DKXp9wxtSUswzd8DXg58cHi9m0mxpk9NMxiPdNpsxh4BC/hmkieTXDv0nVRVrwIMzyeuxgCljWzsEfCCqjqQ5ETg0STPjl3BENhrAY499thlDFHauEYdAavqwPB8EHiQST3Q15JsBxieDy7yWeeGkBYxZnKWdyf5jbdeA38E/AB4iElJerA0vbQsY05BTwIenEwJyFbg76vqkSSPA/cnuQZ4Gbhi9YYpbUxHDOBQgv6cBfp/DFy0GoOSNgvvhJEaGUCpkQGUGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqZEBlBoZQKnRqAAmmUnyQJJnk+xPcn6S45M8muT54fm41R6stNGMPQJ+Hnikqt7HpD7MfixNL01tTFnCY4DfB+4CqKr/rqpDTErT7x7ethv48GoNUtqoxhwBTwdeB76UZF+SO4f6oJaml6Y0JoBbgfcDX6iqc4GfsYTTzSTXJnkiyRNzc3PLHKa0MY0J4CwwW1V7h/YDTAJpaXppSkcMYFX9CHglyZlD10XAM1iaXpra2NmR/hK4J8nRwIvAx5iE19L00hRGBbCqngJ2LbDI0vTSFLwTRmpkAKVGBlBqZAClRgZQamQApUYGUGpkAKVGBlBqZAClRgZQamQApUYGUGpkAKVGBlBqZAClRgZQamQApUZjCvOemeSpeY83k9xgaXppemOqoj1XVTuraifwO8Ac8CCWppemttRT0IuAH1bVf2BpemlqSw3glcCXh9eWppemNDqAQ03Qy4B/WMoKLE0vLW4pR8A/Br5XVa8NbUvTS1NaSgCv4u3TT7A0vTS1sTPkbgMuBr46r/s24OIkzw/Lblv54Ukb29jS9HPACe/o+zGWppem4p0wUiMDKDUygFIjAyg1MoBSIwMoNTKAUiMDKDUygFIjAyg1MoBSIwMoNTKAUiMDKDUygFIjAyg1MoBSIwMoNTKAUiMDKDUygFKjVNXarSx5HfgZ8J9rttK19ZtszG1zu6bz21X13oUWrGkAAZI8UVW71nSla2SjbpvbtXo8BZUaGUCpUUcAv9iwzrWyUbfN7Vola34NKOltnoJKjdY0gEkuSfJckheSrNs55ZOckuSxJPuTPJ3k+qH/+CSPJnl+eD6ue6zLkWRLkn1JHh7apyXZO2zXfcNkretOkpkkDyR5dth353fvszULYJItwN8ymejzbOCqJGev1fpX2GHgE1V1FnAe8PFhW24E9lTVGcCeob0eXQ/sn9f+DPC5YbveAK5pGdX0Pg88UlXvA85hso29+6yq1uQBnA98Y177JuCmtVr/Km/b15jMkfgcsH3o2w481z22ZWzLjuEf4oXAw0CY/LJ660L7cb08gGOAf2f43mNef+s+W8tT0JOBV+a1Z4e+dS3JqcC5wF7gpKp6FWB4PrFvZMt2O/BJ4JdD+wTgUFUdHtrrdb+dDrwOfGk4vb4zybtp3mdrGcAs0Leuv4JN8h7gK8ANVfVm93imleRS4GBVPTm/e4G3rsf9thV4P/CFqjqXyS2R7ZcIaxnAWeCUee0dwIE1XP+KSnIUk/DdU1VvTd39WpLtw/LtwMGu8S3TBcBlSV4C7mVyGno7MJPkrdmU1+t+mwVmq2rv0H6ASSBb99laBvBx4IzhG7WjgSuBh9Zw/SsmSYC7gP1V9dl5ix4Crh5eX83k2nDdqKqbqmpHVZ3KZP98q6o+CjwGfGR427rbLoCq+hHwSpIzh66LgGdo3mdr/dcQH2LyP+oW4O6q+ps1W/kKSvJ7wD8B3+fta6VPM7kOvB/4LeBl4Iqq+knLIKeU5IPAX1XVpUlOZ3JEPB7YB/xZVf28c3zLkWQncCdwNPAi8DEmB6G2feadMFIj74SRGhlAqZEBlBoZQKmRAZQaGUCpkQGUGhlAqdH/AKNxurh7P2C3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation_next, _, _, _ = env.step(1)\n",
    "diff = preprocess_frame(observation_next) - preprocess_frame(observation)\n",
    "plt.imshow(diff.reshape(80,70), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 80 * 70\n",
    "hidden_L1 = 400\n",
    "hidden_L2 = 200\n",
    "actions = [1, 2, 3]\n",
    "n_actions = len(actions)\n",
    "model = {}\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('L1', reuse = False):\n",
    "    init_W1 = tf.truncated_normal_initializer(mean = 0, stddev = 1./np.sqrt(input_dim),\n",
    "                                              dtype = tf.float32)\n",
    "    model['W1'] = tf.get_variable(\"W1\",[input_dim, hidden_L1], initializer = init_W1)\n",
    "    \n",
    "with tf.variable_scope('L2', reuse = False):\n",
    "    init_W2 = tf.truncated_normal_initializer(mean = 0, stddev = 1./np.sqrt(hidden_L1),\n",
    "                                              dtype = tf.float32)\n",
    "    model['W2'] = tf.get_variable(\"W2\",[hidden_L1, n_actions], initializer = init_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    x = tf.matmul(x, model['W1'])\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.matmul(x,model['W2'])\n",
    "    p = tf.nn.softmax(x)\n",
    "    return p\n",
    "\n",
    "def discounted_rewards(reward, gamma):\n",
    "    discounted_function = lambda a,v: a * gamma + v\n",
    "    reward_reverse = tf.scan(discounted_function,\n",
    "                            tf.reverse(reward, [True, False]))\n",
    "    discounted_reward = tf.reverse(reward_reverse, [True, False])\n",
    "    return discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow placeholders\n",
    "episode_x = tf.placeholder(dtype = tf.float32, shape = [None, input_dim])\n",
    "episode_y = tf.placeholder(dtype = tf.float32, shape = [None, n_actions])\n",
    "episode_reward = tf.placeholder(dtype = tf.float32, shape = [None, 1])\n",
    "\n",
    "episode_discounted_reward = discounted_rewards(episode_reward, gamma)\n",
    "episode_mean, episode_variance = tf.nn.moments(episode_discounted_reward, [0], shift = None)\n",
    "\n",
    "#Normalize discounted reward\n",
    "episode_discounted_reward -= episode_mean\n",
    "episode_discounted_reward /= tf.sqrt(episode_variance + 1e-6)\n",
    "\n",
    "#Optimizer settings\n",
    "tf_aprob = policy_forward(episode_x)\n",
    "loss = tf.nn.l2_loss(episode_y - tf_aprob)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients = optimizer.compute_gradients(loss\n",
    "                                        , var_list = tf.trainable_variables()\n",
    "                                        , grad_loss = episode_discounted_reward)\n",
    "train_op = optimizer.apply_gradients(gradients)\n",
    "\n",
    "#Initialize graph\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "#Settings to save the trained model\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "save_path = 'checkpoints/pong_r1.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0: reward: -20\n",
      "episode  1: reward: -21\n",
      "episode  2: reward: -21\n",
      "episode  3: reward: -21\n",
      "episode  4: reward: -21\n",
      "episode  5: reward: -21\n",
      "episode  6: reward: -21\n",
      "episode  7: reward: -21\n",
      "episode  8: reward: -20\n",
      "episode  9: reward: -18\n",
      "Save best model 10: -20.50000 (reward window)\n",
      "episode 10: reward: -20\n",
      "episode 11: reward: -21\n",
      "episode 12: reward: -19\n",
      "episode 13: reward: -19\n",
      "episode 14: reward: -20\n",
      "episode 15: reward: -21\n",
      "episode 16: reward: -21\n",
      "episode 17: reward: -21\n",
      "episode 18: reward: -20\n",
      "episode 19: reward: -20\n",
      "Save best model 20: -20.35000 (reward window)\n",
      "episode 20: reward: -21\n",
      "episode 21: reward: -20\n",
      "episode 22: reward: -21\n",
      "episode 23: reward: -19\n",
      "episode 24: reward: -21\n",
      "episode 25: reward: -19\n",
      "episode 26: reward: -19\n",
      "episode 27: reward: -18\n",
      "episode 28: reward: -21\n",
      "episode 29: reward: -19\n",
      "Save best model 30: -20.16667 (reward window)\n",
      "episode 30: reward: -18\n",
      "episode 31: reward: -18\n",
      "episode 32: reward: -21\n",
      "episode 33: reward: -21\n",
      "episode 34: reward: -21\n",
      "episode 35: reward: -21\n",
      "episode 36: reward: -21\n",
      "episode 37: reward: -21\n",
      "episode 38: reward: -18\n",
      "episode 39: reward: -19\n",
      "Save best model 40: -20.10000 (reward window)\n",
      "episode 40: reward: -17\n",
      "episode 41: reward: -21\n",
      "episode 42: reward: -20\n",
      "episode 43: reward: -20\n",
      "episode 44: reward: -20\n",
      "episode 45: reward: -21\n",
      "episode 46: reward: -20\n",
      "episode 47: reward: -21\n",
      "episode 48: reward: -20\n",
      "episode 49: reward: -21\n",
      "episode 50: reward: -21\n",
      "episode 51: reward: -21\n",
      "episode 52: reward: -21\n",
      "episode 53: reward: -21\n",
      "episode 54: reward: -21\n",
      "episode 55: reward: -20\n",
      "episode 56: reward: -21\n",
      "episode 57: reward: -19\n",
      "episode 58: reward: -21\n",
      "episode 59: reward: -21\n",
      "episode 60: reward: -18\n",
      "episode 61: reward: -21\n",
      "episode 62: reward: -20\n",
      "episode 63: reward: -21\n",
      "episode 64: reward: -18\n",
      "episode 65: reward: -20\n",
      "episode 66: reward: -20\n",
      "episode 67: reward: -20\n",
      "episode 68: reward: -21\n",
      "episode 69: reward: -21\n",
      "episode 70: reward: -19\n",
      "episode 71: reward: -21\n",
      "episode 72: reward: -21\n",
      "episode 73: reward: -19\n",
      "episode 74: reward: -20\n",
      "episode 75: reward: -21\n",
      "episode 76: reward: -21\n",
      "episode 77: reward: -21\n",
      "episode 78: reward: -20\n",
      "episode 79: reward: -20\n",
      "episode 80: reward: -21\n",
      "episode 81: reward: -21\n",
      "episode 82: reward: -19\n",
      "episode 83: reward: -21\n",
      "episode 84: reward: -20\n",
      "episode 85: reward: -21\n",
      "episode 86: reward: -20\n",
      "episode 87: reward: -21\n",
      "episode 88: reward: -21\n",
      "episode 89: reward: -20\n",
      "episode 90: reward: -20\n",
      "episode 91: reward: -18\n",
      "episode 92: reward: -21\n",
      "episode 93: reward: -20\n",
      "episode 94: reward: -20\n",
      "episode 95: reward: -21\n",
      "episode 96: reward: -21\n",
      "episode 97: reward: -20\n",
      "episode 98: reward: -20\n",
      "episode 99: reward: -21\n",
      "episode 100: reward: -21\n",
      "episode 101: reward: -20\n",
      "episode 102: reward: -20\n",
      "episode 103: reward: -21\n",
      "episode 104: reward: -21\n",
      "episode 105: reward: -20\n",
      "episode 106: reward: -20\n",
      "episode 107: reward: -20\n",
      "episode 108: reward: -21\n",
      "episode 109: reward: -20\n",
      "episode 110: reward: -21\n",
      "episode 111: reward: -21\n",
      "episode 112: reward: -21\n",
      "episode 113: reward: -21\n",
      "episode 114: reward: -21\n",
      "episode 115: reward: -19\n",
      "episode 116: reward: -20\n",
      "episode 117: reward: -21\n",
      "episode 118: reward: -20\n",
      "episode 119: reward: -20\n",
      "episode 120: reward: -19\n",
      "episode 121: reward: -20\n",
      "episode 122: reward: -20\n",
      "episode 123: reward: -21\n",
      "episode 124: reward: -21\n",
      "episode 125: reward: -21\n",
      "episode 126: reward: -21\n",
      "episode 127: reward: -21\n",
      "episode 128: reward: -19\n",
      "episode 129: reward: -21\n",
      "episode 130: reward: -21\n",
      "episode 131: reward: -21\n",
      "episode 132: reward: -20\n",
      "episode 133: reward: -21\n",
      "episode 134: reward: -20\n",
      "episode 135: reward: -20\n",
      "episode 136: reward: -19\n",
      "episode 137: reward: -21\n",
      "episode 138: reward: -20\n",
      "episode 139: reward: -21\n",
      "episode 140: reward: -18\n",
      "episode 141: reward: -19\n",
      "episode 142: reward: -16\n",
      "episode 143: reward: -21\n",
      "episode 144: reward: -21\n",
      "episode 145: reward: -20\n",
      "episode 146: reward: -20\n",
      "episode 147: reward: -21\n",
      "episode 148: reward: -19\n",
      "episode 149: reward: -21\n",
      "episode 150: reward: -20\n",
      "episode 151: reward: -21\n",
      "episode 152: reward: -21\n",
      "episode 153: reward: -21\n",
      "episode 154: reward: -21\n",
      "episode 155: reward: -21\n",
      "episode 156: reward: -20\n",
      "episode 157: reward: -20\n",
      "episode 158: reward: -21\n",
      "episode 159: reward: -21\n",
      "episode 160: reward: -21\n",
      "episode 161: reward: -21\n",
      "episode 162: reward: -21\n",
      "episode 163: reward: -21\n",
      "episode 164: reward: -20\n",
      "episode 165: reward: -21\n",
      "episode 166: reward: -21\n",
      "episode 167: reward: -21\n",
      "episode 168: reward: -20\n",
      "episode 169: reward: -21\n",
      "episode 170: reward: -21\n",
      "episode 171: reward: -21\n",
      "episode 172: reward: -21\n",
      "episode 173: reward: -21\n",
      "episode 174: reward: -21\n",
      "episode 175: reward: -21\n",
      "episode 176: reward: -19\n",
      "episode 177: reward: -21\n",
      "episode 178: reward: -19\n",
      "episode 179: reward: -20\n",
      "episode 180: reward: -20\n",
      "episode 181: reward: -21\n",
      "episode 182: reward: -21\n",
      "episode 183: reward: -20\n",
      "episode 184: reward: -20\n",
      "episode 185: reward: -21\n",
      "episode 186: reward: -20\n",
      "episode 187: reward: -19\n",
      "episode 188: reward: -21\n",
      "episode 189: reward: -21\n",
      "episode 190: reward: -21\n",
      "episode 191: reward: -21\n",
      "episode 192: reward: -19\n",
      "episode 193: reward: -17\n",
      "episode 194: reward: -20\n",
      "episode 195: reward: -20\n",
      "episode 196: reward: -21\n",
      "episode 197: reward: -21\n",
      "episode 198: reward: -18\n",
      "episode 199: reward: -19\n",
      "episode 200: reward: -21\n",
      "episode 201: reward: -20\n",
      "episode 202: reward: -17\n",
      "episode 203: reward: -20\n",
      "episode 204: reward: -18\n",
      "episode 205: reward: -21\n",
      "episode 206: reward: -17\n",
      "episode 207: reward: -21\n",
      "episode 208: reward: -19\n",
      "episode 209: reward: -21\n",
      "episode 210: reward: -21\n",
      "episode 211: reward: -20\n",
      "episode 212: reward: -18\n",
      "episode 213: reward: -20\n",
      "episode 214: reward: -20\n",
      "episode 215: reward: -21\n",
      "episode 216: reward: -20\n",
      "episode 217: reward: -20\n",
      "episode 218: reward: -19\n",
      "episode 219: reward: -20\n",
      "episode 220: reward: -18\n",
      "episode 221: reward: -18\n",
      "episode 222: reward: -21\n",
      "episode 223: reward: -21\n",
      "episode 224: reward: -21\n",
      "episode 225: reward: -19\n",
      "episode 226: reward: -19\n",
      "episode 227: reward: -21\n",
      "episode 228: reward: -19\n",
      "episode 229: reward: -20\n",
      "episode 230: reward: -20\n",
      "episode 231: reward: -20\n",
      "episode 232: reward: -20\n",
      "episode 233: reward: -19\n",
      "episode 234: reward: -21\n",
      "episode 235: reward: -20\n",
      "episode 236: reward: -18\n",
      "episode 237: reward: -21\n",
      "episode 238: reward: -21\n",
      "episode 239: reward: -19\n",
      "Save best model 240: -20.07000 (reward window)\n",
      "episode 240: reward: -19\n",
      "episode 241: reward: -21\n",
      "episode 242: reward: -21\n",
      "episode 243: reward: -19\n",
      "episode 244: reward: -21\n",
      "episode 245: reward: -21\n",
      "episode 246: reward: -21\n",
      "episode 247: reward: -20\n",
      "episode 248: reward: -21\n",
      "episode 249: reward: -18\n",
      "episode 250: reward: -20\n",
      "episode 251: reward: -20\n",
      "episode 252: reward: -20\n",
      "episode 253: reward: -18\n",
      "episode 254: reward: -20\n",
      "episode 255: reward: -18\n",
      "episode 256: reward: -21\n",
      "episode 257: reward: -19\n",
      "episode 258: reward: -20\n",
      "episode 259: reward: -20\n",
      "WARNING:tensorflow:From C:\\Users\\auste\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Save best model 260: -20.02000 (reward window)\n",
      "episode 260: reward: -19\n",
      "episode 261: reward: -20\n",
      "episode 262: reward: -18\n",
      "episode 263: reward: -19\n",
      "episode 264: reward: -20\n",
      "episode 265: reward: -20\n",
      "episode 266: reward: -18\n",
      "episode 267: reward: -20\n",
      "episode 268: reward: -19\n",
      "episode 269: reward: -21\n",
      "Save best model 270: -19.88000 (reward window)\n",
      "episode 270: reward: -21\n",
      "episode 271: reward: -19\n",
      "episode 272: reward: -18\n",
      "episode 273: reward: -21\n",
      "episode 274: reward: -20\n",
      "episode 275: reward: -21\n",
      "episode 276: reward: -20\n",
      "episode 277: reward: -17\n",
      "episode 278: reward: -21\n",
      "episode 279: reward: -21\n",
      "Save best model 280: -19.82000 (reward window)\n",
      "episode 280: reward: -20\n",
      "episode 281: reward: -21\n",
      "episode 282: reward: -21\n",
      "episode 283: reward: -18\n",
      "episode 284: reward: -20\n",
      "episode 285: reward: -21\n",
      "episode 286: reward: -21\n",
      "episode 287: reward: -20\n",
      "episode 288: reward: -19\n",
      "episode 289: reward: -21\n",
      "Save best model 290: -19.80000 (reward window)\n",
      "episode 290: reward: -21\n",
      "episode 291: reward: -21\n",
      "episode 292: reward: -21\n",
      "episode 293: reward: -18\n",
      "episode 294: reward: -21\n",
      "episode 295: reward: -20\n",
      "episode 296: reward: -20\n",
      "episode 297: reward: -20\n",
      "episode 298: reward: -20\n",
      "episode 299: reward: -21\n",
      "episode 300: reward: -21\n",
      "episode 301: reward: -21\n",
      "episode 302: reward: -20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 303: reward: -19\n",
      "episode 304: reward: -21\n",
      "episode 305: reward: -19\n",
      "episode 306: reward: -20\n",
      "episode 307: reward: -21\n",
      "episode 308: reward: -21\n",
      "episode 309: reward: -21\n",
      "episode 310: reward: -19\n",
      "episode 311: reward: -20\n",
      "episode 312: reward: -17\n",
      "episode 313: reward: -21\n",
      "episode 314: reward: -19\n",
      "episode 315: reward: -20\n",
      "episode 316: reward: -20\n",
      "episode 317: reward: -17\n",
      "episode 318: reward: -20\n",
      "episode 319: reward: -17\n",
      "episode 320: reward: -20\n",
      "episode 321: reward: -18\n",
      "episode 322: reward: -21\n",
      "episode 323: reward: -21\n",
      "episode 324: reward: -21\n",
      "episode 325: reward: -20\n",
      "episode 326: reward: -21\n",
      "episode 327: reward: -21\n",
      "episode 328: reward: -20\n",
      "episode 329: reward: -20\n",
      "episode 330: reward: -19\n",
      "episode 331: reward: -21\n",
      "episode 332: reward: -21\n",
      "episode 333: reward: -21\n",
      "episode 334: reward: -21\n",
      "episode 335: reward: -20\n",
      "episode 336: reward: -20\n",
      "episode 337: reward: -20\n",
      "episode 338: reward: -20\n",
      "episode 339: reward: -19\n",
      "episode 340: reward: -21\n",
      "episode 341: reward: -21\n",
      "episode 342: reward: -21\n",
      "episode 343: reward: -21\n",
      "episode 344: reward: -21\n",
      "episode 345: reward: -21\n",
      "episode 346: reward: -17\n",
      "episode 347: reward: -21\n",
      "episode 348: reward: -19\n",
      "episode 349: reward: -17\n",
      "episode 350: reward: -21\n",
      "episode 351: reward: -20\n",
      "episode 352: reward: -20\n",
      "episode 353: reward: -17\n",
      "episode 354: reward: -19\n",
      "episode 355: reward: -19\n",
      "episode 356: reward: -20\n",
      "episode 357: reward: -20\n",
      "episode 358: reward: -21\n",
      "episode 359: reward: -20\n",
      "episode 360: reward: -21\n",
      "episode 361: reward: -19\n",
      "episode 362: reward: -18\n",
      "episode 363: reward: -21\n",
      "episode 364: reward: -21\n",
      "episode 365: reward: -21\n",
      "episode 366: reward: -21\n",
      "episode 367: reward: -21\n",
      "episode 368: reward: -18\n",
      "episode 369: reward: -20\n",
      "episode 370: reward: -20\n",
      "episode 371: reward: -20\n",
      "episode 372: reward: -21\n",
      "episode 373: reward: -21\n",
      "episode 374: reward: -19\n",
      "episode 375: reward: -20\n",
      "episode 376: reward: -19\n",
      "episode 377: reward: -20\n",
      "episode 378: reward: -21\n",
      "episode 379: reward: -21\n",
      "episode 380: reward: -21\n",
      "episode 381: reward: -21\n",
      "episode 382: reward: -21\n",
      "episode 383: reward: -21\n",
      "episode 384: reward: -19\n",
      "episode 385: reward: -16\n",
      "episode 386: reward: -20\n",
      "episode 387: reward: -19\n",
      "episode 388: reward: -18\n",
      "episode 389: reward: -18\n",
      "episode 390: reward: -20\n",
      "episode 391: reward: -19\n",
      "episode 392: reward: -18\n",
      "episode 393: reward: -20\n",
      "episode 394: reward: -19\n",
      "episode 395: reward: -20\n",
      "episode 396: reward: -17\n",
      "episode 397: reward: -21\n",
      "episode 398: reward: -18\n",
      "episode 399: reward: -19\n",
      "episode 400: reward: -20\n",
      "episode 401: reward: -21\n",
      "episode 402: reward: -19\n",
      "episode 403: reward: -19\n",
      "episode 404: reward: -17\n",
      "episode 405: reward: -19\n",
      "episode 406: reward: -21\n",
      "episode 407: reward: -19\n",
      "episode 408: reward: -20\n",
      "episode 409: reward: -18\n",
      "Save best model 410: -19.73000 (reward window)\n",
      "episode 410: reward: -21\n",
      "episode 411: reward: -20\n",
      "episode 412: reward: -21\n",
      "episode 413: reward: -20\n",
      "episode 414: reward: -18\n",
      "episode 415: reward: -15\n",
      "episode 416: reward: -21\n",
      "episode 417: reward: -21\n",
      "episode 418: reward: -19\n",
      "episode 419: reward: -21\n",
      "episode 420: reward: -21\n",
      "episode 421: reward: -21\n",
      "episode 422: reward: -21\n"
     ]
    }
   ],
   "source": [
    "obs_prev = None\n",
    "xs, ys, rs = [], [], []\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "reward_window = None\n",
    "reward_best = -22\n",
    "history = []\n",
    "\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    # if True: env.render()\n",
    "    # uncomment if want to see the agent play while training\n",
    "    \n",
    "    #Preprocesses the observation, set input to network to be difference image\n",
    "    obs_cur = preprocess_frame(observation)\n",
    "    obs_diff = obs_cur - obs_prev if obs_prev is not None else np.zeros(input_dim)\n",
    "    obs_prev = obs_cur\n",
    "    \n",
    "    #Sample an action (policy)\n",
    "    feed = {episode_x: np.reshape(obs_diff, (1, -1))}\n",
    "    aprob = sess.run(tf_aprob, feed)\n",
    "    aprob = aprob[0,:]\n",
    "    action = np.random.choice(n_actions, p = aprob)\n",
    "    label = np.zeros_like(aprob)\n",
    "    label[action] = 1\n",
    "    \n",
    "    #Return action to environment and extract next observation, reward, and status\n",
    "    observation, reward, done, info = env.step(action + 1)\n",
    "    reward_sum += reward\n",
    "    #record game history\n",
    "    xs.append(obs_diff)\n",
    "    ys.append(label)\n",
    "    rs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        history.append(reward_sum)\n",
    "        reward_window = -21 if reward_window is None else np.mean(history[-100:])\n",
    "        #Update the weights with the stored values (update policies)\n",
    "        feed = {episode_x: np.vstack(xs)\n",
    "                , episode_y:np.vstack(ys)\n",
    "                , episode_reward: np.vstack(rs)}\n",
    "        _ = sess.run(train_op, feed)\n",
    "        print('episode {:2d}: reward: {:2.0f}'.format(episode_number, reward_sum))\n",
    "        xs, ys, rs = [], [], []\n",
    "        episode_number += 1\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        #Save best model every 10 eps\n",
    "        if (episode_number % 10 == 0) and (reward_window > reward_best):\n",
    "            saver.save(sess, save_path, global_step = episode_number)\n",
    "            reward_best = reward_window\n",
    "            print('Save best model {:2d}: {:2.5f} (reward window)'.format(episode_number, reward_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-35c2708a67e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#Return action to environment and extract next observation, reward, and status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing against OpenAI's Gym\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    if True:\n",
    "        env.render()\n",
    "    #Preprocess the observation, set input to network to be difference image\n",
    "    obs_cur = preprocess_frame(observation)\n",
    "    obs_diff = obs_cur - obs_prev if obs_prev is not None else np.zeros(input_dim)\n",
    "    obs_prev = obs_cur\n",
    "    \n",
    "    #Sample an action (policy)\n",
    "    feed = {episode_x: np.reshape(obs_diff, (1, -1))}\n",
    "    aprob = sess.run(tf_aprob, feed)\n",
    "    aprob = aprob[0,:]\n",
    "    action = np.random.choice(n_actions, p = aprob)\n",
    "    label = np.zeros_like(aprob)\n",
    "    label[action] = 1\n",
    "    \n",
    "    #Return action to environment and extract next observation, reward, and status\n",
    "    observation, reward, done, info = env.step(action + 1)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
